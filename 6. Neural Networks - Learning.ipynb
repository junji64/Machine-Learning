{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망(Neural Networks) : 학습(Learning)\n",
    "## 비용함수(Cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neural Network (Classification)\n",
    "\n",
    "<img src=\"./images/4-layers.png\" width=\"300\" align=\"left\">\n",
    "\n",
    "<br>\n",
    "$$ \\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)}),\n",
    "\\} $$\n",
    "\n",
    "$L = $ 네트워크 내의 전체 레이어 수\n",
    "\n",
    "$s_l = $ 레이어 $l$ 내의 (바이어스 유닛을 제외) 유닛의 수 \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**이진(Binary) 분류 (2 클래스)**:  $y = 0 $ or $1$\n",
    "\n",
    "* 출력층 유닛 : 1개\n",
    "\n",
    "\n",
    "**다중(Multi-class) 분류 (K 클래스)**\n",
    "\n",
    "$y \\in \\mathbb{R}^K \\quad $ e.g. $ \\quad \\begin{bmatrix} 1\\\\0\\\\0\\\\0 \\end{bmatrix},\\quad  \\begin{bmatrix} 0\\\\1\\\\0\\\\0 \\end{bmatrix},\\quad \\begin{bmatrix} 0\\\\0\\\\1\\\\0 \\end{bmatrix},\\quad \\begin{bmatrix} 0\\\\0\\\\0\\\\1 \\end{bmatrix}$\n",
    "\n",
    "* 출력층 유닛 : K개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비용함수(Cost function)\n",
    "\n",
    "로지스틱 회귀:\n",
    "\n",
    "$$J(\\theta) = -{1 \\over m} \\left[  \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1- y^{(i)}) \\log(1-h_\\theta(x^{(i)}) ) \\right] + {\\lambda \\over 2m} \\sum_{j=1}^n \\theta_j^2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "신경망(뉴럴 네트워크):\n",
    "$$h_\\theta(x) \\in \\mathbb{R}^K \\quad (h_\\theta(x))_i = i^{th} \\text{output}$$\n",
    "\n",
    "$$J(\\theta) = -{1 \\over m} \\left[  \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} \\log (h_\\theta(x^{(i)}))_k + (1- y_k^{(i)}) \\log(1-(h_\\theta(x^{(i)})_k) ) \\right] \\\\\n",
    " + {\\lambda \\over 2m} \\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} (\\Theta_{ji}^{(l)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파(Backpropagation) algorithm\n",
    "\n",
    "#### Gradient computation\n",
    "\n",
    "$$J(\\theta) = -{1 \\over m} \\left[  \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} \\log (h_\\theta(x^{(i)}))_k + (1- y_k^{(i)}) \\log(1-(h_\\theta(x^{(i)})_k) ) \\right] \\\\+ {\\lambda \\over 2m} \\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} (\\Theta_{ji}^{(l)})^2$$\n",
    "\n",
    "$$\\min_{\\theta} J(\\theta)$$\n",
    "\n",
    "다음을 계산할 코드가 필요:\n",
    "$$ J(\\theta)$$\n",
    "\n",
    "$${\\partial \\over \\partial \\Theta_{ij}^{(l)}} J(\\theta) \\quad \\text{where } \\Theta_{ij}^{(l)} \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient computation\n",
    "주어진 하나의 훈련자료 $(x,y)$에 대하여:\n",
    "\n",
    "전방 전파(Forward propagation):\n",
    "<img src=\"./images/4-layers.png\" width=\"300\" align=\"right\">\n",
    "\n",
    "$a^{(1)} = x \\quad ( \\text{ add }a_0^{(1)} )$ \n",
    "\n",
    "$z^{(2)} = \\Theta^{(1)} a^{(1)}$\n",
    "\n",
    "$a^{(2)} = g(z^{(2)}) \\quad ( \\text{ add }a_0^{(2)} )$\n",
    "\n",
    "$z^{(3)} = \\Theta^{(2)} a^{(2)}$\n",
    "\n",
    "$a^{(3)} = g(z^{(3)}) \\quad (  \\text{ add }a_0^{(3)}) $\n",
    "\n",
    "$z^{(4)} = \\Theta^{(3)} a^{(3)}$\n",
    "\n",
    "$a^{(4)} = h_\\theta(x) = g(z^{(4)}) $\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 그라디언트 계산 : 역전파(Backpropagation):\n",
    "\n",
    "직관: $\\delta_j^{(l)} = $ 레이어 $l$ 내의 노드 $j$ 의 \"에러\".\n",
    "\n",
    "<img src=\"./images/4-layers.png\" width=\"300\" align=\"right\">\n",
    "\n",
    "레이어 $L=4$ 의 출력층의 각 유닛에 대하여:\n",
    "\n",
    "$\\delta_j^{(4)} = a_j^{(4)} - y_j$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\delta^{(3)} = (\\Theta^{(3)})^T \\delta^{(4)} .* g'(z^{(3)})$ \n",
    "\n",
    "where $ g'(z^{(3)}) = a^{(3)}.*(1-a^{(3)})$ \n",
    "\n",
    "$\\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)} .* g'(z^{(2)}) $\n",
    "\n",
    "NO $\\delta^{(1)}$\n",
    "\n",
    "<br>\n",
    "$$ {\\partial \\over \\partial \\Theta_{ij}^{(l)}} J(\\theta) = a_j^{(l)} \\delta_i^{(l+1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 역전파 알고리즘\n",
    "\n",
    "훈련자료 $ \\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)}),  \\} $\n",
    "\n",
    "Set $\\Delta_{ij}^{(l)} = 0 \\quad (\\text{ for all }l, i, j)$ \n",
    "\n",
    "For $i = 1 \\text{ to } m$\n",
    "\n",
    "$\\quad \\text{Set } a^{(1)} = x^{(i)} $\n",
    "\n",
    "$\\quad a^{(l)} \\text{ 계산을 위해서 순방향 전파 시행 } (\\text{ for } l=2,3,\\cdots, L)$\n",
    "\n",
    "$\\quad y^{(i)} \\text{ 를 사용하여 } \\delta^{(L)} = a^{(L)} -y^{(i) }\\text{ 계산 }$\n",
    "\n",
    "$\\quad \\delta^{(L-1)}, \\delta^{(L-2)}, \\cdots, \\delta^{(2)} \\text{ 계산 }$\n",
    "\n",
    "$\\quad \\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)} $\n",
    "\n",
    "$ D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} +\\lambda \\Theta_{ij}^{(l)} \\text{ if } j \\ne 0 $ \n",
    "\n",
    "$ D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} \\text{ if } j = 0 $ \n",
    "\n",
    "$${\\partial \\over \\partial \\Theta_{ij}^{(l)}} J(\\theta) = D_{ij}^{(l)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 직관\n",
    "\n",
    "<img src=\"./images/forward-prop.png\" width=\"400\">\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전방향 전파 (Forward Propagation)\n",
    "\n",
    "<img src=\"./images/forward-prop2.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "<br>\n",
    "Given $(x^{(i)},y^{(i)}) \\quad z_1^{(3)} = \\Theta_{10}^{(2)} * 1 + \\Theta_{11}^{(2)}  * a_1^{(2)} +\\Theta_{12}^{(2)} * a_2^{(2)} $\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What is backpropagation doing?**\n",
    "\n",
    "$$J(\\theta) = -{1 \\over m} \\left[  \\sum_{i=1}^m  y^{(i)} \\log (h_\\theta(x^{(i)})) + (1- y^{(i)}) \\log(1-(h_\\theta(x^{(i)})) ) \\right] \\\\ + {\\lambda \\over 2m} \\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} (\\Theta_{ji}^{(l)})^2$$\n",
    "\n",
    "<br>\n",
    "\n",
    "출력유닛이 1인 한 예, $x^{(i)}, y^{(i)}$ 에 집중하고, 정규화를 무시하자 ($\\lambda=0$),\n",
    "\n",
    "$$ cost(i) =  -y^{(i)} \\log h_\\theta(x^{(i)}) - (1- y^{(i)}) \\log (1-h_\\theta(x^{(i)})) $$\n",
    "\n",
    "<br>\n",
    "\n",
    "(단순한 개념 이해를 위해서 $ cost(i) \\approx ( h_\\theta(x^{(i)}) - y^{(i)} )^2 $ 라고 생각하는 것도 바람직하다. )     \n",
    "I.e. 네트워크가 예제 i에 대해서 얼마나 잘 수행하고 있습니까?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 역전파 (BackPropagation)\n",
    "\n",
    "\n",
    "<img src=\"./images/forward-prop3.png\" width=\"600\">\n",
    "\n",
    "$\\delta_j^{(l)} = $ \"error\" of cost for $a_j^{(l)}$ ( unit $j$ in layer $l$   ). \n",
    "\n",
    "Formally, $\\delta_j^{(l)} = {\\partial \\over \\partial z_j^{(l)}} \\text{cost}(i)$  (for $j \\ge 0$), where \n",
    "\n",
    "$ \\text{cost}(i) =  y^{(i)} \\log h_\\theta(x^{(i)}) + (1- y^{(i)}) \\log h_\\theta(x^{(i)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking\n",
    "\n",
    "<img src=\"./images/gradient_checking.PNG\" width=\"500\">\n",
    "\n",
    "구현:\n",
    "```\n",
    "gradApprox = (J(theta+EPSILON) - J(theta-EPSILON)) / (2 * EPSILON)\n",
    "```\n",
    "\n",
    "#### 매개변수 벡터: $\\theta$\n",
    "\n",
    "$ \\theta \\in \\mathscr{R}^n$  (E.g. $\\theta$ is the elements of $\\Theta^{(1)}, \\Theta^{(2)},\\Theta^{(3)}$,)\n",
    "\n",
    "$\\theta = \\theta_1,  \\theta_3, \\theta_3,\\cdots, \\theta_n$\n",
    "\n",
    "$$ { \\partial \\over \\partial \\theta_1 } J(\\theta) \\approx {j(\\theta_1 + \\epsilon, \\theta_2, \\theta_3, \\cdots, \\theta_n) - j(\\theta_1 - \\epsilon, \\theta_2, \\theta_3, \\cdots, \\theta_n) \\over 2 \\epsilon }$$\n",
    "\n",
    "$$ { \\partial \\over \\partial \\theta_2 } J(\\theta) \\approx {j(\\theta_1, \\theta_2 + \\epsilon, \\theta_3, \\cdots, \\theta_n) - j(\\theta_1, \\theta_2 - \\epsilon, \\theta_3, \\cdots, \\theta_n) \\over 2 \\epsilon }$$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$$ { \\partial \\over \\partial \\theta_n } J(\\theta) \\approx {j(\\theta_1, \\theta_2, \\theta_3, \\cdots, \\theta_n + \\epsilon) - j(\\theta_1, \\theta_2, \\theta_3, \\cdots,  \\theta_n - \\epsilon) \\over 2 \\epsilon }$$\n",
    "\n",
    "\n",
    "```\n",
    "for i = 1:n,\n",
    "    thetaPlus = theta;\n",
    "    thetaPlus(i) = thetsPlus(i) + EPSILON;\n",
    "    thetaMinus = theta;\n",
    "    thetaMinus(i) = thetsMinus(i) - EPSILON;\n",
    "    gradApprox(i) = ( J(thetaPlus) - J(thetaMinus) ) / ( 2 * EPSILON);\n",
    "end;\n",
    "```\n",
    "$\\text{gradApprox} \\approx DVec $ 를 확인한다.\n",
    "\n",
    "#### 구현 노트 \n",
    "- DVec을 계산하기 위해 역전파를 구현합니다.\n",
    "- gradApprox를 계산하기 위해 수치 그라디언트 검사를 구현하십시오.\n",
    "- 그것들이 비슷한 값을 제공하는지 확인하십시오.\n",
    "- 그래디언트 검사를 끕니다. 학습에 역전파 코드 사용\n",
    "\n",
    "**주의**\n",
    "- 분류자를 훈련시키기 전에 그라디언트 검사 코드를 비활성화하십시오. 그래디언트 디센트의 모든 반복에서 (또는 costFunction (…)의 내부 루프에서) 수치 그래디언트 계산을 실행하면 코드가 매우 느려집니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무작위 초기화(Random initialization)\n",
    "\n",
    "### $\\theta$ 의 초기값\n",
    "\n",
    "경사 하강 및 고급 최적화 방법의 경우 $\\theta$의 초기 값이 필요합니다.\n",
    "\n",
    "#### 0으로 초기화\n",
    "\n",
    "<img src=\"./images/network-2.PNG\" width=\"300\" align=\"left\" >\n",
    "\n",
    "$\\quad \\Theta_{ij}^{(l)} = 0 \\text{ for all } i,j,l $\n",
    "\n",
    "$\\quad a_1^{(2)} = a_2^{(2)}  $\n",
    "\n",
    "$\\quad \\delta_1^{(2)} = \\delta_2^{(2)}  $\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "$ {\\partial \\over \\partial \\Theta_{01}^{(1)} } J (\\theta) ={\\partial \\over \\partial \\Theta_{02}^{(1)} } J (\\theta) $\n",
    "\n",
    "<br>\n",
    "$  \\Theta_{01}^{(1)} = \\Theta_{02}^{(1)}  $\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "각 업데이트 후,두 개의 은닉층 단위 각각에 들어가는 입력에 해당하는 매개 변수는 동일합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 무작위 초기화 : 대칭 파괴(Symmetry breaking)\n",
    "\n",
    "각 $\\Theta_{ji}^{(l)}$ 를 $[-\\epsilon, \\epsilon]$ 에 해당하는 무작위 값으로 초기화 합니다.\n",
    "(i.e. $ -\\epsilon \\le \\Theta_{ji}^{(l)} \\le \\epsilon$ )\n",
    "\n",
    "E.g.\n",
    "```\n",
    "Theta1 = np.random.rand(10,11)* 2 * INIT_EPSILON - INIT_EPSILON;\n",
    "Theta2 = np.random.rand(1,11)* 2 * INIT_EPSILON - INIT_EPSILON;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13486594 -0.0853921   0.19258017 -0.02586717  0.17687425  0.14745247\n",
      "  0.10020735  0.15560329  0.06497756 -0.07675774  0.03892366]\n",
      "[[-0.10257374  0.19076465  0.01394836 -0.04379767 -0.15168472  0.0225423\n",
      "   0.01026747 -0.01896232 -0.09872743 -0.13060336 -0.10980879]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "INIT_EPSILON = 0.2\n",
    "\n",
    "Theta1 = np.random.rand(10,11)* 2 * INIT_EPSILON - INIT_EPSILON;\n",
    "Theta2 = np.random.rand(1,11)* 2 * INIT_EPSILON - INIT_EPSILON;\n",
    "\n",
    "print(Theta1[0,:])\n",
    "print(Theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두 함께 모으기\n",
    "\n",
    "#### 신경망 훈련하기\n",
    "\n",
    "네트워크 아키텍처(뉴런 간의 연결 패턴) 선택 \n",
    "\n",
    "<img src=\"./images/NN_architectures.PNG\" width=\"600\" >\n",
    "\n",
    "- 입력 노드 수 : 특징 $x^{()}$ 의 개수\n",
    "- 출력 노드 수 : 클래스 수\n",
    "- 합리적인 기본값 :  1 개 또는 > 1 인 은닉층으로, 모든 증의 은닉 노드는 같은 수 (일반적으로 더 많으면 더 나음)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 훈련하기\n",
    "\n",
    "1. 가중치를 무작위로 초기화\n",
    "2. 임의의 $x^{(i)}$ 에 대한 $h_\\theta(x^{(i)})$ 를 얻기 위한 전방향 전파를 구현\n",
    "3. 비용 함수 $J(\\theta)$ 를 계산하기위한 코드 구현\n",
    "4. 편미분 함수 $ { \\partial \\over \\partial \\theta_{jk}^{(l)} }J(\\theta)$  를 계산하기 위해 역전파 구현\n",
    "```\n",
    "for i=1:m {\n",
    "    $(x^{(i)},y^{(i)})$를 사용한 전방향 전파와 역전파 수행\n",
    "    $l = 1, \\cdots, L$에 해당하는 활성값 $a^{(l)}$과 오차 $\\delta^{(l)}$를 계산.\n",
    "}\n",
    "그라디언트 계산\n",
    "```\n",
    "\n",
    "5. 그라디언트 검사를 사용하여 역 전파를 사용하여 계산 한 $ { \\partial \\over \\partial \\theta_{jk}^{(l)} }J(\\theta)$ 과 그라디언트의 수치 추정을 사용하여 비교합니다. 그런 다음 그라디언트 검사 코드를 비활성화하십시오.\n",
    "6. 역전파를 사용한 경사하강 또는 고급 최적화 방법을 사용하여 매개 변수$\\theta$ 의 함수 $J(\\theta)$ 를 최소화 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/global_min.PNG\" width=\"600\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파 예 : 자동 주행\n",
    "<img src=\"./images/auto_driving.png\" width=\"600\" >\n",
    "\n",
    "[Link to Video]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

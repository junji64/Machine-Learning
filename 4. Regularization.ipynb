{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Regularization\n* Overfitting\n* Cost Function for Regularization\n* Gradient Descent for Regularized Linear Regression\n* Gradient Descent for Regularized Logistic Regression "
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### 과적합(overfitting)\n\n#### 예: 선형회귀(집 가격 문제)\n\n| underfit, High bias | Just Right  |Overfit, High variance |\n|---------|--------|--------|\n|<img src=\"./images/reg-graph.PNG\" width=\"180\"> |<img src=\"./images/reg-graph.PNG\" width=\"180\"> | <img src=\"./images/reg-graph.PNG\" width=\"180\">|\n|$\\theta_0 + \\theta_1 x$ | $\\theta_0 + \\theta_1 x +\\theta_2 x^2$ | $\\theta_0 + \\theta_1 x +\\theta_2 x^2 +\\theta_3 x^3 +\\theta_4 x^4$ |\n\n\n**과적합(Overfitting)**: 특징이 너무 많으면, 학습된 가설(hypothesis)은\n훈련자료에 아주 잘 일치할 것이다,\n$J(\\theta) =  { 1 \\over 2m} \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2 \\approx 0$\n그러나, 새로운 자료에 대한 일반화(새로운 자료에 대한 가격예측)는 실패한다. \n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n#### 예: 로지스틱 회귀\n\n| underfit, High bias | Just Right  |Overfit, Hight variance |\n|---------|--------|--------|\n|<img src=\"./images/nonlinear data.png\" width=\"180\"> |<img src=\"./images/nonlinear data.png\" width=\"180\"> | <img src=\"./images/nonlinear data.png\" width=\"180\">|\n|$h_\\theta(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)$ | $g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 +\\theta_3 x_1^2 + \\theta_4 x_2^2 + \\theta_5 x_1 x_2)$ | $g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 +\\theta_3 x_1^2 x_2 + \\theta_4 x_1^2 x_2^2 + \\theta_5 x_1^2 x_2^3 + \\theta_6 x_1^3 x_2 + \\cdots )$ |\n\n여기에서 $g = $ sigmoid function\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 과적합(overfitting) 문제 해결\n<img src=\"./images/reg-graph.PNG\" width=\"180\" align=\"right\">\n\n$x_1 =$ 집 크기\n\n$x_2 =$ 방의 개수\n\n$x_3 =$ 층의 개수\n\n$x_4 =$ 집의 나이\n\n$x_5 =$ 이웃의 평균 수입\n\n$x_6 =$ 부엌 크기\n\n$\\vdots$\n\n$x_{100}$\n>\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 과적합 문제 해결\n\nOptions : \n1. **특징 개수 줄이기**\n  \n  \n  * 사용할 특징들은 인위적으로 선택한다.\n  \n  \n  * 모델 선택 알고리즘.\n  \n  \n2. **정규화(Regularization)** (모든 특징들이 의미있고 사용하기를 원한다면)\n  \n  \n  * 모든 특징들을 유지하지만, 파라메타 $\\theta_j$의 크기/값을 감소시킨다.\n  \n  \n  * 각각의 특징들이 $y$를 예측하는데 조금씩 기여하는 많은 특징들을 가질 때 잘 작동된다."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 비용함수(Cost function)\n\n#### 직관(Intuition)\n\n| ---- | ---- |\n|---------|--------|\n|<img src=\"./images/reg-graph.PNG\" width=\"200\"> |<img src=\"./images/reg-graph.PNG\" width=\"200\"> | \n| $\\theta_0 + \\theta_1 x +\\theta_2 x^2$ | $\\theta_0 + \\theta_1 x +\\theta_2 x^2 +\\theta_3 x^3 +\\theta_4 x^4$ |\n\n우리가 벌칙을 가하여 $\\theta_3, \\theta_4$ 를 정말 작게 만든다고 가정하자.\n$$ \\min_\\theta \\left [ { 1 \\over 2m} \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2  + 1000\\cdot \\theta_3^2 + 1000\\cdot \\theta_4^2 \\right ]$$\n$$ \\Rightarrow \\theta_3 \\approx 0, \\theta_4 \\approx 0$$\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 정규화(Regularization)\n\n파라메타들 $\\theta_1, \\cdots, \\theta_n$에 대하여 작은 값들\n* “더 단순한” 가설(hypothesis)\n* 과적합될 경향이 더 적음\n\nHousing:\n* Features: $x_1, x_2, \\cdots, x_{100}$\n* Parameters: $\\theta_1, \\theta_2, \\cdots, \\theta_{100}$\n\n$$J(\\theta) =  { 1 \\over 2m} \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2 + \\lambda \\sum_{j=1}^n \\theta_j^2 $$\n\n$$ \\min_\\theta J(\\theta) \\Rightarrow \\theta_0, \\{ \\theta_1, \\cdots \\theta_n \\} \\approx 0 $$\n\n\n<br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### 정규화(Regularization)\n\n\n$$J(\\theta) =  { 1 \\over 2m} \\left[ \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2 +   \\lambda \\sum_{j=1}^n \\theta_j^2 \\right]$$\nwhere $\\lambda $ : Regularization Parameter\n\n$$ \\min_\\theta J(\\theta) $$\n\n<img src=\"./images/reg-graph.PNG\" width=\"180\">\n\n>\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "정규화된 선형회귀에서, 우리는 다음을 최소화하는 $\\theta$ 를 선택한다.\n\n$$J(\\theta) =  { 1 \\over 2m} \\left[ \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2 +   \\lambda \\sum_{j=1}^n \\theta_j^2 \\right ] $$\n\n만약, $\\lambda$ 가 지극히 큰 값으로 설정되면 (아마도 우리 문제에 너무 큰, 예를 들어 $\\lambda = 10^{10}$)?\n\n* 알고리즘이 잘 작동합니다. 크게 설정하는 것은 문제가 되지 않습니다.\n* Algortihm은 과적합을 제거하지는 못합니다.\n* 알고리즘은 미적합(Underfitting)을 만들게 됩니다. (훈련 데이터에도 잘 맞지 않음).\n* 경사하강이 수렴하지 않습니다.\n\n<img src=\"./images/reg-graph.PNG\" width=\"160\" align=\"left\">\n\n$$h_\\theta(x) = \\theta_0 + \\theta_1 x +\\theta_2 x^2 +\\theta_3 x^3 +\\theta_4 x^4$$\n<br>\n$$  \\{ \\theta_1, \\cdots \\theta_n \\} \\approx 0 $$\n<br>\n\n$$h_\\theta(x) = \\theta_0 $$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Regularized linear regression\n\n\n$$J(\\theta) =  { 1 \\over 2m} \\left[ \\sum_{i=1}^m (h_\\theta(x^{(i)})- y^{(i)})^2 +   \\lambda \\sum_{j=1}^n \\theta_j^2 \\right] $$\n\n$$ \\min_\\theta J(\\theta) $$\n\n>\n\n>\n\n>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Gradient descent for Linear regression\n\nrepeat {\n    \n$\\quad \\theta_0 := \\theta_0 - \\alpha { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) $\n    \n$\\quad \\theta_j := \\theta_j - \\alpha { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) \\cdot x_j^{(i)} $\n$  \\quad \\quad \\quad \\quad \\quad \\quad (j=1,2,3,\\cdots,n)$\n\n}\n\n#### Gradient descent for Regularized Linear regression\n\nrepeat {\n    \n$\\quad \\theta_0 := \\theta_0 - \\alpha { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) $\n    \n$\\quad \\theta_j := \\theta_j - \\alpha \\left[ { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) \\cdot x_j^{(i)} + \\lambda { 1 \\over m} \\theta_j \\right] $\n$  \\quad \\quad \\quad \\quad \\quad \\quad  (j=1,2,3,\\cdots,n)$\n\n}\n\n**Intuition** : \n$\\theta_j := \\theta_j(1 - \\alpha { \\lambda \\over m})- \\alpha { 1 \\over m}\\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) \\cdot x_j^{(i)}  $\n\nwhere $(1 - \\alpha { \\lambda \\over m}) \\approx 1, \\text {but}  < 1 $\n\n<br>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Regularized logistic regression\n\n#### 정규화된 로지스틱 회귀\n<img src=\"./images/nonlinear data.png\" width=\"200\">\n\n$h_{\\theta}(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^2 +\\theta_3 x_1^2 x_2 + \\theta_4 x_1^2 x_2^ + \\theta_5 x_1^2 x_2^3 + \\cdots )$\n\n<div clear=\"both\"> </div>\n\n\n**Cost function:**\n$$J(\\theta) =  -\\left[ { 1 \\over m} \\sum_{i=1}^m y^{(i)}\\log h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) \\log(1-h_{\\theta}(x^{(i)}))    \\right] + \\\\   {\\lambda \\over 2 m} \\sum_{j=1}^n \\theta_j^2 $$\n\n<br>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "**Gradient descent** for Regularized Logistic Regression\n\nrepeat {\n    \n$\\quad \\theta_0 := \\theta_0 - \\alpha { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) x_0^{(i)} $\n    \n$\\quad \\theta_j := \\theta_j - \\alpha \\left[ { 1 \\over m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) -y^{(i)}) \\cdot x_j^{(i)} + { \\lambda \\over m} \\theta_j \\right]   \\quad (j=1,2,3,\\cdots,n)$\n\n}\n\n<br>\n\n<br>\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
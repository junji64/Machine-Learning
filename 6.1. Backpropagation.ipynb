{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Gradient calculation using Backpropagation \n\n신경망 학습에서는 훈련 자료를 입력으로 전방향 전파 후에 역전파를 통해서 경사하강을 위한 그레디언트를 구하게 된다. 이러한 방법은 계산 그래프(computation graph)를 통해서 설명이 가능하다.\n\n### Computation Graph\n\n최소화 시키려는 비용함수가 $J(a,b,c) = 3(a + bc)$ 이라 하면, 이 계산은 다음과 같이 3단계로 구성된다.:\n<br>\n\n1. $u = bc$\n\n2. $v = a + u$\n\n3. $J = 3v$\n\n계산 그래프(computation graph) 형식으로는, \n\n<img src=\"./images/computation_graph.png\" width=\"400\">\n\n위의 계산 그래프 상에서, 주어진 $a,c,b$ 에 대한 비용함수 $J()$의 계산은 왼쪽에서부터 오른쪽으로 계산을 진행하면 되며, 비용함수 $J()$ 의 각 입력 $a,b,c$에 대한 편미분은 거꾸로 오른쪽으로부터 왼쪽으로 계산을 해야 한다는 것을 보일 수 있다.   "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Derivatives with Computation Graph\n\n#### Computing derivatives\n\n<img src=\"./images/computation_graph.png\" width=\"400\">\n\n$ J = 3v \\quad | \\quad  v=11 \\rightarrow 11.001 \\quad | \\quad   J= 33 \\rightarrow 33.003 $\n\n$$ {dJ\\over dv}=  3 $$\n\n\n$  a=5 \\rightarrow 5.001 \\quad | \\quad  v = 11 \\rightarrow 11.001 \\quad | \\quad J= 33 \\rightarrow 33.003 $\n\n$$ {dJ\\over da}= 3 = {dJ\\over dv}{dv\\over da} = 3 \\times 1 $$\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Derivatives with Computation Graph\n\n#### Computing derivatives (계속)\n\n<img src=\"./images/computation_graph.png\" width=\"400\">\n\n$ u = 6 \\rightarrow 6.001 \\quad | \\quad v = 11 \\rightarrow 11.001 \\quad | \\quad J = 33\\rightarrow 33.003$\n\n$$ {dJ\\over du}= 3 = {dJ\\over dv}{dv\\over du}= 3 \\times 1$$\n\n$ b = 3 \\rightarrow 3.001 \\quad | \\quad u = bc = 6 \\rightarrow 6.002 \\quad | \\quad v = 11 \\rightarrow 11.002 \\quad | \\quad J = 33\\rightarrow 33.006$\n\n$$ {dJ\\over db}= 6 = {dJ\\over du}{du\\over db}= 3 \\times 2$$\n\n$$ {dJ\\over dc}= 9 = {dJ\\over du}{du\\over dc}= 3 \\times 3$$\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Logistic Regression Gradient Descent\n\n$ z = w^T x + b$\n\n$ \\hat{y} = a = \\sigma (z)$\n\n$ \\mathscr{L} (a, y) = - \\left[ y\\log(a) + (1-y) \\log(1-a)\\right]$\n\nIn computation graph\n\n<img src=\"./images/computation_graph_lr.jpeg\" width=\"450\">\n\n$$ da ={\\partial \\mathscr{L} \\over \\partial a} = - {y \\over a} + {1-y \\over 1-\na}$$\n\n$$ dz ={\\partial \\mathscr{L} \\over \\partial z} ={\\partial \\mathscr{L} \\over \\partial a} {\\partial a \\over \\partial z}= (-{y \\over a} + {1-y \\over 1-\na} ) a (1 -a) = a - y$$\n\n$$dw_1 = {\\partial \\mathscr{L} \\over \\partial w_1} = {\\partial \\mathscr{L} \\over \\partial z} {\\partial z \\over \\partial w_1} = {\\partial \\mathscr{L} \\over \\partial z}  x_1 = (a-y) x_1$$\n\n$$dw_2 = {\\partial \\mathscr{L} \\over \\partial w_2} = {\\partial \\mathscr{L} \\over \\partial z} {\\partial z \\over \\partial w_2} = {\\partial \\mathscr{L} \\over \\partial z}  x_2 = (a-y) x_2$$\n\n$$db ={\\partial \\mathscr{L} \\over \\partial b} = {\\partial \\mathscr{L} \\over \\partial z} {\\partial z \\over \\partial d}= {\\partial \\mathscr{L} \\over \\partial z} = (a-y)$$\n\n$ w_1 := w_1 - \\alpha \\cdot dw_1$\n\n$ w_2 := w_2 - \\alpha \\cdot dw_2$\n\n$ b := b - \\alpha \\cdot db$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Gradient descent on m examples\n\n#### Logistic regression on m examples\n\n$$ J(w,b) = {1 \\over m} \\sum_{i=1}^m \\mathscr{L} (a^{(i)}, y^{(i)}) $$\n\nwhere $ a^{(i)} = \\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\sigma(w^T x^{(i)} + b) $\n\n주어진 $(x^{(i)},y^{(i)})$ 에 대하여,  $dw_1^{(i)},dw_2^{(i)},db^{(i)}$ 를 역전파 알고리즘으로 계산한다. \n\n$$ {\\partial J(w,b) \\over \\partial w_1} = {1 \\over m} \\sum_{i=1}^m  {\\partial \\over \\partial w_1} \\mathscr{L} (a^{(i)}, y^{(i)}) = {1 \\over m} \\sum_{i=1}^m  d w_1^{(i)} $$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Algorithm in Pseudo-code\n\n$ J = 0; \\quad dw_1 = 0; \\quad dw_2 = 0; \\quad db=0$\n\nFor $i = 1$ to $m$\n\n$\\quad z^{(i)} = w^T x^{(i)} + b$\n\n$\\quad a^{(i)} = \\sigma (z^{(i)})$\n\n$\\quad J += -\\left[ y^{(i)} \\log a^{(i)} +  (1- y^{(i)}) \\log (1 - a^{(i)}) \\right ]$\n\n$\\quad dz^{(i)} = a^{(i)} - y^{(i)}$\n\n$\\quad dw_1 += x_1^{(i)} dz^{(i)}$\n\n$\\quad dw_2 += x_2^{(i)} dz^{(i)}$\n\n$\\quad db += dz^{(i)}$\n\n$J = J / m ; \\quad dw_1= dw_1/m ; \\quad  dw_2 =  dw_2 / m ; \\quad db =db/ m;$\n\n$ w_1 := w_1 - \\alpha dw_1$\n\n$ w_2 := w_2 - \\alpha dw_2$\n\n\n$ b := b - \\alpha db$\n\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}